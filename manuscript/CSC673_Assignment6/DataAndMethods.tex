\section{Data and Methods}\label{sec:data\_and\_methods}

\subsection{General Architecture of the Framework}
Figure 1 demonstrates the implementation of our workflow. The first step of our workflow, input data, asks the user to input their data. While our workflow was developed specifically in reference to “The Attack on America and Civil Liberties Trade-Offs: A Three-Wave National Panel Survey, 2001-2004”, many aspects of the workflow are fully reusable for other datasets. 

In the second stage, the user sets their specifications that corresponds to the type of experiment they would like to run choosing from one of the following options: exploration, preliminary, baseline, or augmentation. For each of the experiment choice, a result file will be output in this own directory in the output directory for examination. 

In the following workflow step, data processing choice, the user specifies what data type they would like to store their data in if they want their data transformed, and if or how they want their data to be augmented. If the experiment type is exploration, user can choose whatever combination to explore. If the choice is preliminary, the workflow will automatically executes experiments for discovery of batch effects. Baseline and augmentation choice will enable the workflow to automatically combine all data processing choice and perform benchmarking studies. 

In data processors, the data moves through the data auditing pipeline, the data cleaning pipeline, and the feature selection pipeline. Next is the modeling stage, where all of the classifiers are trained with the data input from the data processor stage. During these two stages, users can also set their preferred data processing threshold and hyperparameter tuning values in the configuration file. 

After the classifiers have been trained the best classifier can be selected from the results. A cycle in the workflow was drawn to show that the workflow was developed so that users could iteratively develop classifiers with different inputs and specifications to yield a high performance model with high fidelity data. 

\subsection{Data Description}
The original shape of our dataset is 3361 by 787 where there are 3361 samples and 787 features. After processing our data and preparing it for our classifiers, the size of our dataset decreases significantly. In Table 1, we compare the size of our training dataset across data augmentation methods with and without our proposed data transformation specific to the dataset. Without the transformation, the dataset contains many more samples. At the baseline comparison, the untransformed dataset has near three times the samples as the transformed dataset. While the ratio between the number of untransformed training dataset size and transformed training dataset size varies, the untransformed dataset contains significantly more samples across all data augmentation methods. Interestingly, since we employed a variation of data augmentation methods, including undersampling, not all data augmentation methods resulted in a training dataset larger than the baseline. In editENN and SMOTEEEN, the size of the training dataset decreased for the transformed iteration. We also found a existence of batch effect in our dataset. Batch effect occurs when variations in data that are not related to their actual labels but a dominant feature, underlying structure make it difficult to analyze the data of interest. 

\subsection{Data Transformation}
To transform the original data format into a format well suited for classifiers and remove batch effects, we decreased the number of attributes associated with each respondent. In its original form, each sample is associated with 787 attributes. However, 91 wave dependent variables record the same data for different waves of the survey. To reduce the number of attributes we condensed these three sets of wave dependent attributes into one set of wave dependent variables. This left us with 515 attributes per column. With 515 attributes we are able to represent a respondent and one of the respondents responses in a survey wave. Rather than keep all of the survey waves for each respondent, we only kept the wave in which the respondent participated. This left us with a much denser dataset. Rather than having 91 to 192 attributes filled with null values for each respondent, we instead have 515 attributes with few null values. After the data transformation, the shape of our transformed dataset is 3661 by 515.

\subsection{Data Processing}
During data processing, the data moves through three pipelines ultimately yielding the processed data in four formats for the user to choose from, Raw data, PCA components, UMAP components, UMAP\_PCA components. Our pipelines are designed to serve a specific function, where the first pipeline is for auditing the data, the second pipeline for cleaning the data, and the last pipeline for feature selection. Our data audit pipeline is designed to produce a report to the user with useful information about the data. The pipeline contains the following five steps: note duplicate rows, note duplicate columns, note columns with a null percentage above a user defined threshold, and note rows with a null percentage above a user defined threshold. All the irregular indices of rows and columns are stored in a dictionary that could be later accessed by the data cleaning pipeline. Although our data audit pipeline makes note of useful information for the user it does not make any changes to the data.

In our data cleaning pipeline, we make changes to the data that were identified in our data audit pipeline. The data cleaning pipeline is made up of four steps as follows: delete columns with a null percentage above a user defined threshold, delete duplicate columns and rows, and delete outlier rows with a user defined standard deviation threshold. After our data has been cleaned, we send our data to our final pipeline, the feature selection pipeline. We defined the standard deviation threshold at 3 standard deviations and the maximum null percentage at 0.4 for all the experiments.

The feature selection pipeline takes input from data cleaning pipeline and is made up of the following steps: stratify balance data upon a user defined label column, remove correlated features above a user defined threshold by computing the correlation matrix, exclude the one feature from the correlating pair, normalize the columns using Min-Max scaling, and perform variable feature selection obtaining the top standard deviation of 100 features. If the size of the features is smaller than 100, the original features are obtained. After the data has passed through the feature selection pipeline, it will be ready to be processed as Raw data, or be converted to either PCA or UMAP components. In our implementation we balance the data upon W3IWER4\_A which is the target variable. We set the feature correlation upper bound at 0.98 and set the number of variable features to select to 100.

Ultimately, the user has four choices on the format of the data which they would like to process. As mentioned previously the four formats are Raw, PCA components, UMAP components, UMAP\_PCA components. We included PCA components to allow users to try to find linear patterns in their data. Additionally, we included UMAP components to allow users to find non-linear patterns in their data. We also included UMAP\_PCA components since this is often a technique in exploratory data analysis. Lastly, we included Raw data for datasets like ours that were small enough to be processed without dimensionality reduction. We provide different means to provide user a better flexibility to explore the relationship within the dataset and to improve the visualizations. 

\subsection{Data Augmentation Methods}
In our study, we explore various data augmentation methods to address the issue of class imbalance in our training dataset. Five primary sampling-based techniques are employed, including over-sampling, under-sampling, and hybrid-sampling methods. Additionally, we integrate a Generative Adversarial Network (GAN) with each method, effectively doubling the size of the training dataset. Therefore, in total, we tested 10 data augmentation methods on our dataset.

\subsubsection{Synthetic Minority Over-sampling Technique (SMOTE)}
SMOTE is a widely-used benchmarking approach for upsampling imbalanced datasets ~\cite{SMOTE_Kegelmeyer_2002}. It works by creating synthetic samples for the minority class, thus balancing the distribution of classes. It is an iterative method that first selects a minority class, identifies the k-nearest neighbors, and at last, generates synthetic instances within the same neighborhood.

\subsubsection{Edited Nearest Neighbour (editENN)}
EditENN focuses on improving the quality of the training set by removing instances that may be noise or are likely misclassified ~\cite{Wilson_1972}. It does this by considering the nearest neighbors of each instance. That is, if an instance's class label differs from the majority of its neighbors, it is removed. This method helps in cleaning the data and can lead to a more balanced class distribution. Although editENN modifies the majority class, it preserves the samples in the minority class to maintain its integrity.

\subsubsection{Tomek’s links (TomekLinks)}
Like editENN, Tomek-Links also modify the majority class to improve model generalization ~\cite{Elhassan_M_2016}. Tomek-Links are pairs of samples, one from the majority class and one from the minority class, that are nearest neighbors. After the Tomek-Links have been identified, the Tomek-Links samples in the majority class are removed. The motivation behind removing the majority class Tomek-Links is to increase the separation between the two classes, which can be beneficial for the classifier's performance.

\subsubsection{SMOTE and editENN (SMOTEENN)}
In SMOTEENN, synthetic samples for the minority class is generated by SMOTE. Then, editENN is applied to reduce the noise in the majority class by removing potential misclassified samples. The result is a more balanced dataset where the minority class has been enhanced via synthetic data ~\cite{imbalanceLearn}.

\subsubsection{SMOTE and TomekLinks (SMOTETomek)}
In SMOTETomek, first, SMOTE is applied to generate synthetic instances of the minority class. Then Tomek-Links are identified where the samples in the majority class are removed. Like in SMOTEENN, the result is a more balanced dataset where the separation between classes has been enhanced ~\cite{imbalanceLearn}. 

\subsubsection{Generative Adversarial Networks (GANs)}
Each of the aforementioned methods is further augmented with the use of GANs. GANs are employed to generate synthetic data, which adds variety and volume to the training dataset ~\cite{Goodfellow_2014}. Specifically, GANs have two major components, the generator and the discriminator. During the training, the generator creates synthetic data from random noise, which is then evaluated by the discriminator to determine its authenticity compared to real data. The generator continuously refines its output based on feedback from the discriminator, aiming to produce increasingly synthetic data similar to real data. Simultaneously, the Discriminator improves its ability to differentiate real data from synthetic data by discriminative loss, which is a binary cross-entropy loss function that quantifies how well the Discriminator is performing at correctly classifying real and generated data.

\subsection{Classifiers}
We trained five classifiers of different types and one naive classifier which we used as a baseline to compare other classifiers against. For the naive classifier, we used Sklearn’s Dummy Classifier. The naive classifier classifies every sample as the most frequent class label in the dataset. Since it is a naive classifier we did not tune any hyperparameters; however, we did run 5 cross fold validation tests as we did for every other classifier. Aside from the naive classifier, we trained the following classifiers: Multilayer Perceptron (MLP), Logistic Regression, Decision Tree, Random Forest and XGBoost. 

MLP is a neural network based classifier ~\cite{Rosenblatt_1958}. Like other neural network models, MLP consists of node layers where there is an input layer, some hidden layers, and an output layer. During training, the hidden layers learn the parameter weights via the feedforward and backpropagation architecture. In our implementation, we hyperparameter tune the solver, alpha, and the type of learning rate, where the types of solvers are adam or stochastic gradient descent while  the types of learning rates are constant or adaptive. 

Logistic regression is a linear model based classifier. During training logistic regression adjusts the parameter weights to minimize the difference between predicted labels and the true labels ~\cite{Cox_1958}. Although logistic regression can be applied to non-linear data, it performs strongest when the data is linearly separable. For logistic regression we tune the following hyperparameters: penalty, regularizer, and solver. The potential hyperparameters for penalty are L1, L2, and elastic net and the potential hyperparameters for solver are Newton Conjugate-Gradient, LBFGS, or LIBLINEAR. 

Decision Tree is a tree based classifier ~\cite{Belson_1959}. During training, the decision tree recursively splits nodes until a stopping condition. Unlike other classifiers, decision trees result in a set of decision rules that define the conditions for classifying a sample. In our implementation we tune the following hyperparameters for decision tree: metric and minimum number of samples in each leaf. The metric can be any of the following: gini, entropy, or logarithmic loss. 

Random forest is an ensemble based classifier ~\cite{Morgan_Sonquist_1963}. During training, random forest builds multiple decision trees independently and merges them together. Random forest is more accurate than decision trees and aims to reduce overfitting. We used the exact same hyperparameter tuning variables and values on random forest that we did for decision tree. 

XGBoost is an ensemble learning algorithm made up of a collection of division trees ~\cite{Chen_2016}. XGBoost takes advantage of gradient descent optimization to iteratively improve the model’s accuracy by minimizing the error between the true values and the predicted value. XGBoost also incorporates regularization to prevent it from overfitting. In our implementation we tuned the following hyperparameters: learning rate and subsample.

\subsection{Metrics}
Performance of cooperativeness classifiers is evaluated using the accuracy and weighted F1 score. Accuracy measures the fraction of cooperativeness that are correctly predicted in the entire test dataset (eq ~\ref{eq:accuracy}). It considers the weights of each class in the classification equally and ranges from 0 to 1, with higher values implying a better performance. The formula is as follows, where TP and TN refer to true positive and true negative and FP and FN refer to false positive and false negative.

\begin{equation}\label{eq:accuracy}
    \begin{gathered}
        Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
    \end{gathered}
\end{equation}

The weighted F1 score accounts for the balance of classes in the dataset, which is particularly useful in scenarios where classes are imbalanced. It is calculated by the weighted harmonic mean of precision and recall, giving a better measure of the incorrectly classified cases. It ranges from 0 to 1, where a higher value indicates better performance. The formula is as follow (eq ~\ref{eq:weighted_f1}), where precision is the ratio of true positive predictions to the total positive predictions, and Recall is the ratio of true positive predictions to the total actual positive cases. The ``weighted'' aspect of the F1 score means that each class's F1 score is multiplied by a weight proportional to the number of true instances for each class, and then, these scores are averaged, thereby taking the class imbalance into account.=-
\begin{equation}\label{eq:weighted_f1}
\begin{gathered}
F1 = 2 \times \frac{Precision \times Recall}{Precision + Recall}
\end{gathered}
\end{equation}

\subsection{Implementation}
All the scripts are written in Python Programming Language. Scikit-learn ~\cite{sklearn_api}(version 1.3.2) is primarily used for data processing and model building, where the data processing pipeline also contains our own designed functions. Sampling-based data augmentation is implemented by imbalanced-learn package ~\cite{imbalanceLearn} (version 0.11.0), and the architecture of GANs is adopted from Kaggle ~\cite{SEYEDSAMAN_EMAMI} and implemented using tensorflow ~\cite{tensorflow2015} (version 2.13.1). For data augmentation methods, the default parameters are used. All the experimental results are fully reproducible, and we also provide a framework for easily benchmarking different data augmentation methods and data type. All the scripts, experimental results, and executing instruction are reproducible and available at this GitHub repo: \href{https://github.com/SheltonZhaoK/Mitigating-Batch-Effects-and-Enhancing-Cooperativeness-Classification-in-Human-Survey-Data}{https://github.com/SheltonZhaoK/Mitigating-Batch-Effects-and-Enhancing-Cooperativeness-Classification-in-Human-Survey-Data}.
